% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fn_pplssvm.R
\name{pplssvm}
\alias{pplssvm}
\title{Penalized Principal Least Squares Support Vector Machine (P\eqn{^2}LSSVM) for Sparse Sufficient Dimension Reduction}
\usage{
pplssvm(
  x,
  y,
  H = 10,
  C = 1,
  lambda,
  gamma = 3.7,
  penalty = "grSCAD",
  max.iter = 100
)
}
\arguments{
\item{x}{Numeric predictor matrix (n x p), where n is the sample size and p is the number of predictors.}

\item{y}{Numeric response vector of length n.}

\item{H}{The number of slicing point (default: 10).}

\item{C}{Regularization parameter (default: 1).}

\item{lambda}{Penalty parameter for sparsity.}

\item{gamma}{Regularization parameter for SCAD/MCP penalty (default: 3.7).}

\item{penalty}{Penalty type: \code{"grSCAD"} (default), \code{"grLasso"}, or \code{"grMCP"}.}

\item{max.iter}{Maximum number of iterations for the algorithm (default: 100).}
}
\value{
A list containing:
\item{evalues}{Eigenvalues of the estimated central subspace matrix.}
\item{evectors}{Eigenvectors (columns) corresponding to the estimated sufficient directions.}
\item{x}{The original predictor matrix.}
}
\description{
This function implements the penalized principal least squares support vector machine (P\eqn{^2}LSSVM) approach
for sparse sufficient dimension reduction (SDR).
}
\details{
This method estimates a sparse basis for the central subspace by minimizing a penalized principal least squares SVM objective.
It supports group penalties to induce row-wise sparsity and efficiently computes solutions even for high-dimensional data.
}
\examples{
\dontrun{
  set.seed(1)
  n <- 100; p <- 10
  B <- matrix(0, p, 2)
  B[1,1] <- B[2,2] <- 1
  x <- MASS::mvrnorm(n, rep(0, p), diag(1, p))
  y <- (x \%*\% B[,1] / (0.5 + (x \%*\% B[,2] + 1)^2)) + 0.2 * rnorm(n)
  fit <- pplssvm(x, y, H = 10, C = 1, lambda = 0.03, gamma = 3.7,
               penalty = "grSCAD", max.iter = 100)
  fit$evectors[, 1:2]
}

}
